from fastapi import FastAPI, UploadFile, File, Form
from transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer
import torch
from PIL import Image
import io
from fastapi.responses import StreamingResponse
from transformers import TextIteratorStreamer
import threading

app = FastAPI()

# ëª¨ë¸ ë¡œë”©
model_name = "naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B"
device = "cuda" if torch.cuda.is_available() else "cpu"

model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True).to(device)
preprocessor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(model_name)

@app.post("/infer")
async def infer(prompt: str = Form(...), image: UploadFile = File(None)):
    print(f"ğŸ” ì¼ë°˜ ìš”ì²­: prompt='{prompt[:50]}...', image={bool(image)}")
    
    try:
        # HyperCLOVAX ì˜¬ë°”ë¥¸ ë°©ì‹ìœ¼ë¡œ ì±„íŒ… êµ¬ì„±
        vlm_chat = [
            {"role": "system", "content": {"type": "text", "text": "You are a helpful assistant."}},
            {"role": "user", "content": {"type": "text", "text": prompt}}
        ]
        
        # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ VLM ë°©ì‹ìœ¼ë¡œ ì¶”ê°€
        if image:
            try:
                image_bytes = await image.read()
                print(f"ğŸ“· ì´ë¯¸ì§€ íŒŒì¼ í¬ê¸°: {len(image_bytes)} bytes")
                
                if len(image_bytes) > 0:
                    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (HyperCLOVAXê°€ íŒŒì¼ ê²½ë¡œë‚˜ PIL ê°ì²´ í•„ìš”)
                    import tempfile
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_file:
                        tmp_file.write(image_bytes)
                        tmp_path = tmp_file.name
                    
                    # HyperCLOVAX ë°©ì‹ìœ¼ë¡œ ì´ë¯¸ì§€ ì¶”ê°€
                    vlm_chat.append({
                        "role": "user",
                        "content": {
                            "type": "image",
                            "filename": "uploaded_image.jpg",
                            "image": tmp_path,
                        }
                    })
                    print("âœ… VLM ì±„íŒ…ì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ")
                    
                    # HyperCLOVAX ì˜¬ë°”ë¥¸ ì „ì²˜ë¦¬ ë°©ì‹
                    new_vlm_chat, all_images, is_video_list = preprocessor.load_images_videos(vlm_chat)
                    preprocessed = preprocessor(all_images, is_video_list=is_video_list)
                    
                    # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
                    input_ids = tokenizer.apply_chat_template(
                        new_vlm_chat, 
                        return_tensors="pt", 
                        tokenize=True, 
                        add_generation_prompt=True
                    ).to(device)
                    
                    print("âœ… HyperCLOVAX VLM ì „ì²˜ë¦¬ ì™„ë£Œ")
                    
                    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                    import os
                    os.unlink(tmp_path)
                    
                else:
                    print("âš ï¸ ë¹ˆ ì´ë¯¸ì§€ íŒŒì¼, í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬")
                    # í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬
                    input_ids = tokenizer.apply_chat_template(
                        vlm_chat, 
                        return_tensors="pt", 
                        tokenize=True, 
                        add_generation_prompt=True
                    ).to(device)
                    preprocessed = {}
                    
            except Exception as img_error:
                print(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {str(img_error)}")
                # ì´ë¯¸ì§€ ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬
                input_ids = tokenizer.apply_chat_template(
                    vlm_chat, 
                    return_tensors="pt", 
                    tokenize=True, 
                    add_generation_prompt=True
                ).to(device)
                preprocessed = {}
        else:
            # í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬
            input_ids = tokenizer.apply_chat_template(
                vlm_chat, 
                return_tensors="pt", 
                tokenize=True, 
                add_generation_prompt=True
            ).to(device)
            preprocessed = {}
            print("âœ… í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬")

        # ìƒì„±
        output_ids = model.generate(
            input_ids=input_ids,
            max_new_tokens=256,
            do_sample=True,
            top_p=0.6,
            temperature=0.5,
            repetition_penalty=1.0,
            **preprocessed  # HyperCLOVAX ì „ì²˜ë¦¬ ê²°ê³¼ ì¶”ê°€
        )

        response = tokenizer.decode(output_ids[0], skip_special_tokens=True)
        return {"result": response}
        
    except Exception as e:
        print(f"âŒ ì¼ë°˜ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return {"result": f"Error: {str(e)}"}

@app.post("/infer_stream")
async def infer_stream(prompt: str = Form(...), image: UploadFile = File(None)):
    print(f"ğŸ” ìŠ¤íŠ¸ë¦¼ ìš”ì²­: prompt='{prompt[:50]}...', image={bool(image)}")
    
    try:
        # HyperCLOVAX ì˜¬ë°”ë¥¸ ë°©ì‹ìœ¼ë¡œ ì±„íŒ… êµ¬ì„±
        vlm_chat = [
            {"role": "system", "content": {"type": "text", "text": "You are a helpful assistant."}},
            {"role": "user", "content": {"type": "text", "text": prompt}}
        ]
        
        # ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ VLM ë°©ì‹ìœ¼ë¡œ ì¶”ê°€
        if image:
            try:
                image_bytes = await image.read()
                print(f"ğŸ“· ì´ë¯¸ì§€ íŒŒì¼ í¬ê¸°: {len(image_bytes)} bytes")
                
                if len(image_bytes) > 0:
                    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ (HyperCLOVAXê°€ íŒŒì¼ ê²½ë¡œë‚˜ PIL ê°ì²´ í•„ìš”)
                    import tempfile
                    with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp_file:
                        tmp_file.write(image_bytes)
                        tmp_path = tmp_file.name
                    
                    # HyperCLOVAX ë°©ì‹ìœ¼ë¡œ ì´ë¯¸ì§€ ì¶”ê°€
                    vlm_chat.append({
                        "role": "user",
                        "content": {
                            "type": "image",
                            "filename": "uploaded_image.jpg",
                            "image": tmp_path,
                        }
                    })
                    print("âœ… VLM ì±„íŒ…ì— ì´ë¯¸ì§€ ì¶”ê°€ ì™„ë£Œ")
                    
                    # HyperCLOVAX ì˜¬ë°”ë¥¸ ì „ì²˜ë¦¬ ë°©ì‹
                    new_vlm_chat, all_images, is_video_list = preprocessor.load_images_videos(vlm_chat)
                    preprocessed = preprocessor(all_images, is_video_list=is_video_list)
                    
                    # ì±„íŒ… í…œí”Œë¦¿ ì ìš©
                    input_ids = tokenizer.apply_chat_template(
                        new_vlm_chat, 
                        return_tensors="pt", 
                        tokenize=True, 
                        add_generation_prompt=True
                    ).to(device)
                    
                    print("âœ… HyperCLOVAX VLM ì „ì²˜ë¦¬ ì™„ë£Œ")
                    
                    # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                    import os
                    os.unlink(tmp_path)
                    
                else:
                    print("âš ï¸ ë¹ˆ ì´ë¯¸ì§€ íŒŒì¼, í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬")
                    # í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬
                    input_ids = tokenizer.apply_chat_template(
                        vlm_chat, 
                        return_tensors="pt", 
                        tokenize=True, 
                        add_generation_prompt=True
                    ).to(device)
                    preprocessed = {}
                    
            except Exception as img_error:
                print(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {str(img_error)}")
                # ì´ë¯¸ì§€ ì‹¤íŒ¨ ì‹œ í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬
                input_ids = tokenizer.apply_chat_template(
                    vlm_chat, 
                    return_tensors="pt", 
                    tokenize=True, 
                    add_generation_prompt=True
                ).to(device)
                preprocessed = {}
        else:
            # í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬
            input_ids = tokenizer.apply_chat_template(
                vlm_chat, 
                return_tensors="pt", 
                tokenize=True, 
                add_generation_prompt=True
            ).to(device)
            preprocessed = {}
            print("âœ… í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬")

        # ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
        streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)
        generation_kwargs = dict(
            input_ids=input_ids,
            max_new_tokens=256,
            do_sample=True,
            top_p=0.6,
            temperature=0.5,
            repetition_penalty=1.0,
            streamer=streamer,
            **preprocessed  # HyperCLOVAX ì „ì²˜ë¦¬ ê²°ê³¼ ì¶”ê°€
        )
        
        thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)
        thread.start()

        def token_stream():
            for new_text in streamer:
                yield new_text

        return StreamingResponse(token_stream(), media_type="text/plain")
        
    except Exception as e:
        print(f"âŒ ìŠ¤íŠ¸ë¦¼ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        def error_stream():
            yield f"Error: {str(e)}"
        return StreamingResponse(error_stream(), media_type="text/plain")
