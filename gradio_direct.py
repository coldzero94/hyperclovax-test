import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer, TextIteratorStreamer
from PIL import Image
import threading

class HyperCLOVAXHandler:
    def __init__(self):
        print("ğŸš€ ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.model_name = "naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B"
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name, 
            trust_remote_code=True
        ).to(self.device)
        self.preprocessor = AutoProcessor.from_pretrained(
            self.model_name, 
            trust_remote_code=True
        )
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

    def stream_chat(self, message, history, image=None):
        try:
            # 1. Chat í…œí”Œë¦¿ êµ¬ì„±
            chat = [{"role": "system", "content": "You are a helpful assistant."}]
            for turn in history:
                if turn["role"] != "system":
                    chat.append(turn)
            chat.append({"role": "user", "content": message})

            input_ids = self.tokenizer.apply_chat_template(
                chat, return_tensors="pt", tokenize=True
            ).to(self.device)

            inputs = {"input_ids": input_ids}
            if image:
                image_pil = Image.open(image).convert("RGB")
                pixel_values = self.preprocessor(
                    image_pil, return_tensors="pt"
                ).pixel_values.to(self.device)
                inputs["pixel_values"] = pixel_values

            streamer = TextIteratorStreamer(
                self.tokenizer, 
                skip_special_tokens=True
            )
            generation_kwargs = dict(
                **inputs,
                max_new_tokens=256,
                do_sample=True,
                top_p=0.6,
                temperature=0.5,
                repetition_penalty=1.0,
                streamer=streamer,
            )

            thread = threading.Thread(
                target=self.model.generate, 
                kwargs=generation_kwargs
            )
            thread.start()

            partial = ""
            for new_text in streamer:
                partial += new_text
                yield {"role": "assistant", "content": partial}

        except Exception as e:
            yield {"role": "assistant", "content": f"âŒ Error: {str(e)}"}

def main():
    handler = HyperCLOVAXHandler()

    with gr.Blocks(title="ğŸ¤— HyperCLOVAX Chat", fill_height=True) as demo:
        gr.Markdown(
            "<h2>ğŸ¤— HyperCLOVAX Direct Chat</h2>"
            "<p>Gradioì—ì„œ ì§ì ‘ transformers ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ì‹¤ì‹œê°„ ì±„íŒ… ë°ëª¨ì…ë‹ˆë‹¤.</p>"
        )

        chatbot = gr.Chatbot(type="messages", show_copy_button=True)
        state = gr.State([])  # history as list of {"role": ..., "content": ...}

        with gr.Row():
            txt = gr.Textbox(
                placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš” (Enter: ì „ì†¡, Shift+Enter: ì¤„ë°”ê¿ˆ)",
                lines=2,
                show_label=False
            )
            image = gr.Image(
                type="filepath", label="ì´ë¯¸ì§€ ì—…ë¡œë“œ (ì„ íƒ)", container=True
            )

        with gr.Row():
            send_btn = gr.Button("ğŸ’¬ ì „ì†¡")
            retry_btn = gr.Button("ğŸ”„ ì¬ì‹œë„")
            clear_btn = gr.Button("ğŸ—‘ï¸ ëŒ€í™” ì§€ìš°ê¸°")

        def user_submit(message, history, img_path):
            history = history + [{"role": "user", "content": message}]
            return "", history, handler.stream_chat(message, history, img_path)

        send_btn.click(
            fn=user_submit,
            inputs=[txt, state, image],
            outputs=[txt, state, chatbot]
        )

        def retry_last(history, img_path):
            if not history:
                return history, chatbot
            last_input = next((msg["content"] for msg in reversed(history) if msg["role"] == "user"), "")
            trimmed_history = [msg for msg in history if msg["role"] != "assistant"]
            trimmed_history.append({"role": "user", "content": last_input})
            return trimmed_history, handler.stream_chat(last_input, trimmed_history, img_path)

        retry_btn.click(
            fn=retry_last,
            inputs=[state, image],
            outputs=[state, chatbot]
        )

        clear_btn.click(
            lambda: ([], []),
            outputs=[chatbot, state]
        )

    demo.queue().launch(server_name="0.0.0.0", server_port=7861)

if __name__ == "__main__":
    main()
