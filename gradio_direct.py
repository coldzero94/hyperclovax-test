import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoProcessor, AutoTokenizer, TextIteratorStreamer
from PIL import Image
import threading

class HyperCLOVAXHandler:
    def __init__(self):
        print("ğŸš€ ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.model_name = "naver-hyperclovax/HyperCLOVAX-SEED-Vision-Instruct-3B"
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name, 
            trust_remote_code=True
        ).to(self.device)
        self.preprocessor = AutoProcessor.from_pretrained(
            self.model_name, 
            trust_remote_code=True
        )
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")

    def stream_chat(self, message, history, image=None):
        try:
            # 1. Chat template êµ¬ì„±
            chat = [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": message}
            ]
            input_ids = self.tokenizer.apply_chat_template(
                chat, return_tensors="pt", tokenize=True
            ).to(self.device)

            # 2. ì´ë¯¸ì§€ ì²˜ë¦¬ (ìˆì„ ê²½ìš°)
            inputs = {"input_ids": input_ids}
            if image:
                image_pil = Image.open(image).convert("RGB")
                pixel_values = self.preprocessor(
                    image_pil, return_tensors="pt"
                ).pixel_values.to(self.device)
                inputs["pixel_values"] = pixel_values

            # 3. ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
            streamer = TextIteratorStreamer(
                self.tokenizer, 
                skip_special_tokens=True
            )
            generation_kwargs = dict(
                **inputs,
                max_new_tokens=256,
                do_sample=True,
                top_p=0.6,
                temperature=0.5,
                repetition_penalty=1.0,
                streamer=streamer,
            )
            
            thread = threading.Thread(
                target=self.model.generate, 
                kwargs=generation_kwargs
            )
            thread.start()

            # 4. ìŠ¤íŠ¸ë¦¼ ì‘ë‹µ
            partial = ""
            for new_text in streamer:
                partial += new_text
                yield partial
                
        except Exception as e:
            yield f"âŒ Error: {str(e)}"

def main():
    # ëª¨ë¸ í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”
    handler = HyperCLOVAXHandler()
    
    with gr.Blocks(title="ğŸ¤— HyperCLOVAX Direct Chat", fill_height=True) as demo:
        with gr.Row():
            gr.Markdown(
                "<h2>ğŸ¤— HyperCLOVAX Direct Chat (No API) ğŸ¤—</h2>"
                "<h3>Gradioì—ì„œ ì§ì ‘ transformers ëª¨ë¸ ì‚¬ìš©!</h3>"
                "<h3>âœ¨ FastAPI ì—†ì´ ì§ì ‘ ì—°ê²°ëœ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ âœ¨</h3>"
            )
        
        # ì»¤ìŠ¤í…€ ì±—ë´‡ ìœ„ì ¯
        chatbot = gr.Chatbot(
            type="messages", 
            scale=20, 
            render_markdown=True,
            show_label=False,
            container=True,
            show_copy_button=True
        )
        
        # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
        gr.ChatInterface(
            fn=handler.stream_chat,
            chatbot=chatbot,
            textbox=gr.Textbox(
                placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš” (Enter: ì „ì†¡, Shift+Enter: ì¤„ë°”ê¿ˆ)", 
                lines=2,
                show_label=False
            ),
            additional_inputs=[
                gr.Image(
                    type="filepath", 
                    label="ì´ë¯¸ì§€ ì—…ë¡œë“œ (ì„ íƒ)",
                    container=True
                )
            ],
            submit_btn="ğŸ’¬ ì „ì†¡",
            retry_btn="ğŸ”„ ì¬ì‹œë„", 
            undo_btn="â†©ï¸ ë˜ëŒë¦¬ê¸°",
            clear_btn="ğŸ—‘ï¸ ëŒ€í™” ì§€ìš°ê¸°"
        )

    demo.queue().launch(server_name="0.0.0.0", server_port=7861)

if __name__ == "__main__":
    main() 